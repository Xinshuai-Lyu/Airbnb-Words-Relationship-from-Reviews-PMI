{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2038737\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict,Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\c2038737\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\c2038737\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\c2038737\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "p = os.path.curdir\n",
    "df = pd.read_csv(os.path.join(p,'reviews.csv'))\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_stop_words(stopwords):\n",
    "    \"\"\"Add common punctuation marks to the stopwords\n",
    "    :type stopwords: set\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    stopwords = []\n",
    "    for stop_word in sw:\n",
    "        stopwords.append(stop_word)\n",
    "    stopwords += [\",\", \".\", \"!\", \";\", \"â€™\", \"'\", \":\", \"\\\"\", \"..\", \"...\", \"....\", \"......\", \"+\", \"-\", \"*\"]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "def process_reviews(df):\n",
    "    \"\"\"Process the raw reviews\n",
    "    :type df: dataframe\n",
    "    :rtype: dataframe -- with three extra columns tokenized, tagged, lower_tagged\n",
    "    \"\"\"\n",
    "    comments = df[\"comments\"]\n",
    "    tokenized_comments = []\n",
    "    stopwords = extend_stop_words(sw)\n",
    "    tokenized_list = []\n",
    "    tagged_list = []\n",
    "    lower_tagged_list = []\n",
    "    for comment in comments:\n",
    "        if not comment: \n",
    "            tokenized = \"empty\"\n",
    "            tagged = \"empty\"\n",
    "            lower_tagged = \"empty\"\n",
    "        else:\n",
    "            # Transform comment to be lower form\n",
    "            comment = comment.lower()\n",
    "            # Tokenize a review\n",
    "            tokenized = word_tokenize(comment)\n",
    "            # Tag words so as to differentiate nouns from adjectives or verbs\n",
    "            tagged = pos_tag(tokenized)\n",
    "            # Only remain meaningful words so as to reduce the size of the vocabulary\n",
    "            meaningful_words = []\n",
    "            for tokenized_word in tokenized:\n",
    "                if tokenized_word not in stopwords:\n",
    "                    meaningful_words.append(tokenized_word)\n",
    "            if not meaningful_words: lower_tagged = \"empty\"\n",
    "            else: lower_tagged = pos_tag(meaningful_words)\n",
    "        tokenized_list.append(tokenized)\n",
    "        tagged_list.append(tagged)\n",
    "        lower_tagged_list.append(lower_tagged)\n",
    "    df[\"tokenized\"] = tokenized_list\n",
    "    df[\"tagged\"] = tagged_list\n",
    "    df[\"lower_tagged\"] = lower_tagged_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rGYB8gx5Qq-P"
   },
   "outputs": [],
   "source": [
    "df = process_reviews(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tagged</th>\n",
       "      <th>lower_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "      <td>[daniel, is, really, cool, ., the, place, was,...</td>\n",
       "      <td>[(daniel, NN), (is, VBZ), (really, RB), (cool,...</td>\n",
       "      <td>[(daniel, NN), (really, RB), (cool, JJ), (plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "      <td>[daniel, is, the, most, amazing, host, !, his,...</td>\n",
       "      <td>[(daniel, NN), (is, VBZ), (the, DT), (most, RB...</td>\n",
       "      <td>[(daniel, NN), (amazing, VBG), (host, NN), (pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "      <td>[we, had, such, a, great, time, in, amsterdam,...</td>\n",
       "      <td>[(we, PRP), (had, VBD), (such, JJ), (a, DT), (...</td>\n",
       "      <td>[(great, JJ), (time, NN), (amsterdam, JJ), (da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "      <td>[very, professional, operation, ., room, is, v...</td>\n",
       "      <td>[(very, RB), (professional, JJ), (operation, N...</td>\n",
       "      <td>[(professional, JJ), (operation, NN), (room, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "      <td>[daniel, is, highly, recommended, ., he, provi...</td>\n",
       "      <td>[(daniel, NN), (is, VBZ), (highly, RB), (recom...</td>\n",
       "      <td>[(daniel, NN), (highly, RB), (recommended, VBD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \\\n",
       "0  Daniel is really cool. The place was nice and ...   \n",
       "1  Daniel is the most amazing host! His place is ...   \n",
       "2  We had such a great time in Amsterdam. Daniel ...   \n",
       "3  Very professional operation. Room is very clea...   \n",
       "4  Daniel is highly recommended.  He provided all...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [daniel, is, really, cool, ., the, place, was,...   \n",
       "1  [daniel, is, the, most, amazing, host, !, his,...   \n",
       "2  [we, had, such, a, great, time, in, amsterdam,...   \n",
       "3  [very, professional, operation, ., room, is, v...   \n",
       "4  [daniel, is, highly, recommended, ., he, provi...   \n",
       "\n",
       "                                              tagged  \\\n",
       "0  [(daniel, NN), (is, VBZ), (really, RB), (cool,...   \n",
       "1  [(daniel, NN), (is, VBZ), (the, DT), (most, RB...   \n",
       "2  [(we, PRP), (had, VBD), (such, JJ), (a, DT), (...   \n",
       "3  [(very, RB), (professional, JJ), (operation, N...   \n",
       "4  [(daniel, NN), (is, VBZ), (highly, RB), (recom...   \n",
       "\n",
       "                                        lower_tagged  \n",
       "0  [(daniel, NN), (really, RB), (cool, JJ), (plac...  \n",
       "1  [(daniel, NN), (amazing, VBG), (host, NN), (pl...  \n",
       "2  [(great, JJ), (time, NN), (amsterdam, JJ), (da...  \n",
       "3  [(professional, JJ), (operation, NN), (room, N...  \n",
       "4  [(daniel, NN), (highly, RB), (recommended, VBD...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lower_tagged_to_csv(lower_tagged):\n",
    "    \"\"\"write lower tagged to csv to conquer memory error\n",
    "    :type lower_tagged: series<[(word, pos),(word, pos),...]> \n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(\"./lower_tagged.csv\"):\n",
    "        f = open(\"lower_tagged.csv\", \"a\", encoding=\"utf-8\")\n",
    "        f.write(\"word,pos\\n\")\n",
    "        for list_words_poss in lower_tagged:\n",
    "            for word, pos in list_words_poss: \n",
    "                f.write(word+\",\"+pos[0]+\"\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conquer memory error\n",
    "lower_tagged = df[\"lower_tagged\"][df[\"lower_tagged\"] != \"empty\"].to_numpy()\n",
    "write_lower_tagged_to_csv(lower_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv(\"./lower_tagged.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    \"\"\"Get 1000 most frequent nouns and 1000 most frequent verbs or adjectives\n",
    "    :type df: dataframe \n",
    "    :rtype: tuple<list, list> -- (1000 nouns, 1000 verbs or adjectives)\n",
    "    \"\"\"\n",
    "    # center contains 1000 most frequent nouns\n",
    "    center = vocab_df[\"word\"][vocab_df.pos == \"N\"].value_counts().index[0:1000].to_list()\n",
    "    # context contains 1000 most frequent verbs or adjectives\n",
    "    temp = vocab_df[\"word\"][(vocab_df.pos == \"J\") | ((vocab_df.pos == \"V\"))].value_counts()\n",
    "    context = temp.index[0:1000].to_list()\n",
    "    return center, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "F_R5l4IVSk9-"
   },
   "outputs": [],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_coocs_between_center_and_context(cent_vocab, cont_vocab):\n",
    "    \"\"\"initiate co-occurrences between center and context words\n",
    "    :type cent_vocab: list -- 1000 center words\n",
    "    :type cont_vocab: list -- 1000 context words\n",
    "    :rtype: dict<dict> -- keys in outer dict are center words, \n",
    "    which contains an inner dict whose keys are all context words \n",
    "    and values are initial co-occurrences (0) between center and context words\n",
    "    \"\"\"\n",
    "    inner_dict = {}\n",
    "    for context_word in cont_vocab:\n",
    "        # initiate co-occurrences between center and context word\n",
    "        inner_dict[context_word] = 0\n",
    "    outer_dict = {}\n",
    "    for center_word in cent_vocab:\n",
    "        # initiate co-occurrences between center word and context words\n",
    "        outer_dict[center_word] = inner_dict.copy()\n",
    "    return outer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_noun_or_av_list(lower_tagged):\n",
    "    \"\"\"Split lower_tagged into noun list and adj or verb list\n",
    "    :type lower_tagged: list<tuple> -- [(word, pos), (word, pos), ...]\n",
    "    :rtype: tuple<list, list> -- noun_list, av_list\n",
    "    \"\"\"\n",
    "    noun_list = []\n",
    "    av_list = []\n",
    "    for word, pos in lower_tagged:\n",
    "        if pos[0] == \"N\":\n",
    "            noun_list.append(word)\n",
    "        if pos[0] == \"V\" or pos[0] == \"J\":\n",
    "            av_list.append(word)\n",
    "    return noun_list, av_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ddnfCbQWRd5R"
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "    \"\"\"Count co-occurrences between center and context words\n",
    "    :type df: dataframe\n",
    "    :type df: cent_vocab -- 1000 center words\n",
    "    :type cont_vocab: list -- 1000 context words\n",
    "    :rtype: dict<dict> -- keys in outer dict are center words, \n",
    "    which contains an inner dict whose keys are all context words \n",
    "    and values are co-occurrences between center and context words\n",
    "    \"\"\"\n",
    "    # This trick helps decrease loops \n",
    "    coocs = initiate_coocs_between_center_and_context(cent_vocab, cont_vocab)\n",
    "    lower_taggeds = df[\"lower_tagged\"][df[\"lower_tagged\"] != \"empty\"].to_numpy()\n",
    "    nums = len(lower_taggeds)\n",
    "    i = 0\n",
    "    while i < nums:\n",
    "        lower_tagged = lower_taggeds[i]\n",
    "        # Split it into noun list and adj or verb list\n",
    "        # so as to solve the problem center and context words being \n",
    "        # part of your vocabulary because they are frequent both as a noun and as a verb\n",
    "        noun_list, av_list = split_into_noun_or_av_list(lower_tagged)\n",
    "        # noun_list transformed to set type\n",
    "        # so as to avoid center words occurring more than once\n",
    "        noun_set = set(noun_list)\n",
    "        # I consider the context as the whole review,\n",
    "        # Which means that I believe the vector built by the whole review\n",
    "        # can disclose the meaning of center words by context words\n",
    "        # and also can disclose the relationship between center words and center words\n",
    "        for noun_word in noun_set:\n",
    "            for av_word in av_list:\n",
    "                try:\n",
    "                    # This trick can avoid using \"in\" so as to decrease loops \n",
    "                    # because of the existence of coocs\n",
    "                    coocs[noun_word][av_word] += 1\n",
    "                except:\n",
    "                    # If key error happens, let it continue, does not matter\n",
    "                    pass\n",
    "        i += 1\n",
    "    return coocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iTT_TOkaSoXL"
   },
   "outputs": [],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "    \"\"\"returns a DataFrame where each row corresponds to one center word, \n",
    "and each column corresponds to one context word, \n",
    "and cells are their corresponding co-occurrence value.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(coocs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cwAflxldSrbg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocdf = cooc_dict2df(coocs)\n",
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "    \"\"\"Returns a DataFrame where each cell contains PMI valua\n",
    "    :type df: dataframe\n",
    "    :rtype: DataFrame \n",
    "    \"\"\"\n",
    "    # cumsums of center words\n",
    "    center_cumsums = coocdf.sum(axis=1)\n",
    "    # cumsums of context words\n",
    "    context_cumsums = coocdf.sum(axis=0)\n",
    "    # total\n",
    "    total = center_cumsums.sum() \n",
    "    # (center,context) probability\n",
    "    center_context_probability = df / total\n",
    "    # center probaility\n",
    "    center_probability = np.array(center_cumsums / total)[None] # \"None\" makes the shape as (1,1000) \n",
    "    # context probaility\n",
    "    context_probability = np.array(context_cumsums / total)[None]\n",
    "    # PMI(x,y) = log(p(x,y)/p(x)p(y))\n",
    "    # and if less than 0 then let it be 0\n",
    "    px_py = np.dot(center_probability.T, context_probability) + 10**(-6) # \"10**(-6)\" avoids divide by zero bug\n",
    "    pmidf = np.maximum(np.log(center_context_probability / px_py), 0)\n",
    "    return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AGftXjXRSuQw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-cecdc52a59ec>:21: RuntimeWarning: divide by zero encountered in log\n",
      "  pmidf = np.maximum(np.log(center_context_probability / px_py), 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf = cooc2pmi(coocdf)\n",
    "pmidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `â€˜towelsâ€™`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "    \"\"\"Returns a list of N strings, in order of their PMI score with the center_word\n",
    "    :type df: dataframe\n",
    "    :type center_word: str\n",
    "    :type N: int\n",
    "    :rtype: list \n",
    "    \"\"\"\n",
    "    return pmidf.loc[center_word].sort_values()[-N:].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1I038zG1Sw62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wine',\n",
       " 'cheese',\n",
       " 'provided',\n",
       " 'including',\n",
       " 'fridge',\n",
       " 'fresh',\n",
       " 'microwave',\n",
       " 'nespresso',\n",
       " 'kettle',\n",
       " 'tea']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['given',\n",
       " 'loft',\n",
       " 'table',\n",
       " 'expect',\n",
       " 'advertised',\n",
       " 'good',\n",
       " 'nice',\n",
       " 'adorable',\n",
       " 'cute',\n",
       " 'friendly']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'dog')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
